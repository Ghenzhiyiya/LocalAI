import gradio as gr
import os
import json
import logging
from pathlib import Path
from huggingface_hub import hf_hub_download, list_models, HfApi
from llama_cpp import Llama
import threading
import time
from typing import Optional, List, Dict
from config import *
from datetime import datetime

class ModelManager:
    def __init__(self):
        self.models_dir = Path(DIRECTORY_CONFIG["models_dir"])
        self.models_dir.mkdir(exist_ok=True)
        self.config_file = DIRECTORY_CONFIG["config_file"]
        self.current_model: Optional[Llama] = None
        self.model_info = self.load_model_config()
        self.setup_logging()
        
    def load_model_config(self) -> Dict:
        """åŠ è½½æ¨¡å‹é…ç½®"""
        if os.path.exists(self.config_file):
            with open(self.config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def save_model_config(self):
        """ä¿å­˜æ¨¡å‹é…ç½®"""
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(self.model_info, f, ensure_ascii=False, indent=2)
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logs_dir = Path(DIRECTORY_CONFIG["logs_dir"])
        logs_dir.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(logs_dir / f"localai_{datetime.now().strftime('%Y%m%d')}.log", encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def get_small_models_from_hf(self) -> List[str]:
        """ä»Hugging Faceè·å–å°äº7Bçš„æ¨¡å‹åˆ—è¡¨"""
        try:
            models = list_models(
                filter="gguf",
                sort="downloads",
                direction=-1,
                limit=50
            )
            
            small_models = []
            include_keywords = MODEL_FILTER["include_keywords"]
            exclude_keywords = MODEL_FILTER["exclude_keywords"]
            
            for model in models:
                model_name = model.modelId.lower()
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å°æ¨¡å‹å…³é”®è¯ä¸”ä¸åŒ…å«å¤§æ¨¡å‹å…³é”®è¯
                if any(keyword in model_name for keyword in include_keywords) and \
                   not any(keyword in model_name for keyword in exclude_keywords):
                    small_models.append(model.modelId)
                    if len(small_models) >= DOWNLOAD_CONFIG["max_models_display"]:
                        break
            
            # æ·»åŠ æ¨èæ¨¡å‹
            for model in RECOMMENDED_MODELS:
                if model not in small_models:
                    small_models.append(model)
            
            return small_models[:DOWNLOAD_CONFIG["max_models_display"]]
            
        except Exception as e:
            self.logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return RECOMMENDED_MODELS[:10]
    
    def download_model(self, model_id: str, progress_callback=None) -> str:
        """ä¸‹è½½æ¨¡å‹"""
        try:
            if progress_callback:
                progress_callback(f"å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_id}")
            
            # æŸ¥æ‰¾GGUFæ–‡ä»¶
            from huggingface_hub import HfApi
            api = HfApi()
            
            try:
                files = api.list_repo_files(model_id)
                gguf_files = [f for f in files if f.endswith('.gguf')]
                
                if not gguf_files:
                    raise Exception("æœªæ‰¾åˆ°GGUFæ ¼å¼æ–‡ä»¶")
                
                # é€‰æ‹©æœ€å°çš„GGUFæ–‡ä»¶
                gguf_file = sorted(gguf_files)[0]
                
            except:
                # å¦‚æœæ— æ³•è·å–æ–‡ä»¶åˆ—è¡¨ï¼Œå°è¯•å¸¸è§çš„æ–‡ä»¶å
                possible_files = [
                    "model.gguf",
                    "ggml-model.gguf",
                    f"{model_id.split('/')[-1]}.gguf",
                    "pytorch_model.gguf"
                ]
                gguf_file = possible_files[0]
            
            if progress_callback:
                progress_callback(f"ä¸‹è½½æ–‡ä»¶: {gguf_file}")
            
            # ä¸‹è½½æ¨¡å‹æ–‡ä»¶
            model_path = hf_hub_download(
                repo_id=model_id,
                filename=gguf_file,
                local_dir=self.models_dir / model_id.replace('/', '_'),
                local_dir_use_symlinks=False
            )
            
            # ä¿å­˜æ¨¡å‹ä¿¡æ¯ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
            abs_model_path = os.path.abspath(model_path)
            self.model_info[model_id] = {
                "path": abs_model_path,
                "downloaded": True,
                "file": gguf_file
            }
            self.save_model_config()
            
            if progress_callback:
                progress_callback(f"æ¨¡å‹ä¸‹è½½å®Œæˆ: {model_path}")
            
            return model_path
            
        except Exception as e:
            error_msg = f"ä¸‹è½½æ¨¡å‹å¤±è´¥: {str(e)}"
            if progress_callback:
                progress_callback(error_msg)
            raise Exception(error_msg)
    
    def load_model(self, model_path: str) -> bool:
        """åŠ è½½æ¨¡å‹"""
        try:
            # ç¡®ä¿ä½¿ç”¨ç»å¯¹è·¯å¾„
            if not os.path.isabs(model_path):
                model_path = os.path.abspath(model_path)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(model_path):
                self.logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                return False
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(model_path)
            self.logger.info(f"å‡†å¤‡åŠ è½½æ¨¡å‹: {model_path} (å¤§å°: {file_size/(1024**3):.2f} GB)")
            
            if self.current_model:
                del self.current_model
            
            self.current_model = Llama(
                model_path=model_path,
                n_ctx=MODEL_CONFIG["n_ctx"],
                n_threads=MODEL_CONFIG["n_threads"],
                verbose=MODEL_CONFIG["verbose"]
            )
            self.logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
            return True
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            self.logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {error_details}")
            return False
    
    def generate_response(self, prompt: str, max_tokens: int = None) -> str:
        """ç”Ÿæˆå›å¤"""
        if not self.current_model:
            return "è¯·å…ˆé€‰æ‹©å¹¶åŠ è½½æ¨¡å‹"
        
        if max_tokens is None:
            max_tokens = MODEL_CONFIG["max_tokens"]
        
        try:
            response = self.current_model(
                prompt,
                max_tokens=max_tokens,
                temperature=MODEL_CONFIG["temperature"],
                top_p=MODEL_CONFIG["top_p"],
                repeat_penalty=MODEL_CONFIG["repeat_penalty"],
                echo=False,
                stop=["\nç”¨æˆ·:", "\n\n", "ç”¨æˆ·:", "åŠ©æ‰‹:", "\nåŠ©æ‰‹:"]
            )
            return response['choices'][0]['text'].strip()
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {e}")
            return f"ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {str(e)}"

# å…¨å±€æ¨¡å‹ç®¡ç†å™¨
model_manager = ModelManager()

def get_available_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    return model_manager.get_small_models_from_hf()

def download_and_load_model(model_id: str, progress=gr.Progress()):
    """ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹"""
    try:
        progress(0, desc="å¼€å§‹ä¸‹è½½æ¨¡å‹...")
        
        def progress_callback(msg):
            print(msg)
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½
        if model_id in model_manager.model_info and model_manager.model_info[model_id].get('downloaded'):
            model_path = model_manager.model_info[model_id]['path']
            progress(0.8, desc="æ¨¡å‹å·²å­˜åœ¨ï¼Œæ­£åœ¨åŠ è½½...")
        else:
            progress(0.2, desc="æ­£åœ¨ä¸‹è½½æ¨¡å‹...")
            model_path = model_manager.download_model(model_id, progress_callback)
            progress(0.8, desc="ä¸‹è½½å®Œæˆï¼Œæ­£åœ¨åŠ è½½...")
        
        # åŠ è½½æ¨¡å‹
        if model_manager.load_model(model_path):
            progress(1.0, desc="æ¨¡å‹åŠ è½½å®Œæˆ")
            return f"âœ… æ¨¡å‹ {model_id} åŠ è½½æˆåŠŸ", gr.update(interactive=True)
        else:
            return f"âŒ æ¨¡å‹ {model_id} åŠ è½½å¤±è´¥", gr.update(interactive=False)
            
    except Exception as e:
        return f"âŒ é”™è¯¯: {str(e)}", gr.update(interactive=False)

def chat_response(message, history):
    """èŠå¤©å›å¤å‡½æ•°"""
    if not model_manager.current_model:
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": "è¯·å…ˆé€‰æ‹©å¹¶åŠ è½½æ¨¡å‹"})
        return history
    
    # é™åˆ¶å†å²é•¿åº¦ï¼Œåªä¿ç•™æœ€è¿‘çš„6è½®å¯¹è¯ï¼ˆ12æ¡æ¶ˆæ¯ï¼‰
    if len(history) > 12:
        history = history[-12:]
    
    # æ„å»ºå¯¹è¯å†å²ï¼Œä½¿ç”¨æ›´è‡ªç„¶çš„æ ¼å¼
    conversation = "ä½ æ˜¯ä¸€ä¸ªå‹å¥½ã€æœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚è¯·æ ¹æ®å¯¹è¯å†å²ï¼Œè‡ªç„¶åœ°å›å¤ç”¨æˆ·çš„é—®é¢˜ã€‚\n\n"
    
    # æ·»åŠ å†å²å¯¹è¯
    for msg in history:
        if msg["role"] == "user":
            conversation += f"ç”¨æˆ·: {msg['content']}\n"
        elif msg["role"] == "assistant":
            conversation += f"åŠ©æ‰‹: {msg['content']}\n"
    
    # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
    conversation += f"ç”¨æˆ·: {message}\nåŠ©æ‰‹: "
    
    # ç”Ÿæˆå›å¤
    response = model_manager.generate_response(conversation, max_tokens=256)
    
    # æ¸…ç†å›å¤ï¼Œç§»é™¤å¯èƒ½çš„é‡å¤å†…å®¹
    response = response.replace("åŠ©æ‰‹:", "").replace("ç”¨æˆ·:", "").strip()
    
    # å¦‚æœå›å¤ä¸ºç©ºï¼Œå°è¯•ç”¨æ›´ç®€å•çš„æç¤ºé‡æ–°ç”Ÿæˆ
    if not response or len(response.strip()) < 2:
        simple_prompt = f"è¯·å›å¤ç”¨æˆ·çš„è¯ï¼š{message}"
        response = model_manager.generate_response(simple_prompt, max_tokens=128)
        response = response.strip()
        
        # å¦‚æœä»ç„¶ä¸ºç©ºï¼Œæä¾›å‹å¥½çš„é»˜è®¤å›å¤
        if not response:
            response = "æˆ‘æ­£åœ¨å­¦ä¹ ä¸­ï¼Œè¯·å¤šåŒ…æ¶µã€‚æ‚¨èƒ½æ¢ä¸ªæ–¹å¼é—®é—®å—ï¼Ÿ"
    
    history.append({"role": "user", "content": message})
    history.append({"role": "assistant", "content": response})
    return history

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    theme = getattr(gr.themes, UI_CONFIG["theme"].capitalize(), gr.themes.Soft)()
    
    with gr.Blocks(title=UI_CONFIG["title"], theme=theme) as app:
        gr.Markdown(f"# ğŸ¤– {UI_CONFIG['title']}")
        gr.Markdown(UI_CONFIG["description"])
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("## ğŸ“¥ æ¨¡å‹ç®¡ç†")
                
                model_dropdown = gr.Dropdown(
                    choices=get_available_models(),
                    label="é€‰æ‹©æ¨¡å‹",
                    info="é€‰æ‹©è¦ä¸‹è½½çš„AIæ¨¡å‹ï¼ˆå°äº7Bå‚æ•°ï¼‰"
                )
                
                download_btn = gr.Button("ğŸ“¥ ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹", variant="primary")
                model_status = gr.Textbox(
                    label="æ¨¡å‹çŠ¶æ€",
                    value="æœªåŠ è½½æ¨¡å‹",
                    interactive=False
                )
                
                gr.Markdown("## âš™ï¸ è®¾ç½®")
                refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°æ¨¡å‹åˆ—è¡¨")
                
            with gr.Column(scale=2):
                gr.Markdown("## ğŸ’¬ å¯¹è¯")
                
                chatbot = gr.Chatbot(
                    height=UI_CONFIG["chatbot_height"],
                    placeholder="é€‰æ‹©å¹¶åŠ è½½æ¨¡å‹åå¼€å§‹å¯¹è¯...",
                    type="messages"
                )
                
                with gr.Row():
                    msg_input = gr.Textbox(
                        placeholder="è¾“å…¥æ‚¨çš„æ¶ˆæ¯...",
                        scale=4,
                        interactive=False
                    )
                    send_btn = gr.Button("å‘é€", scale=1, interactive=False)
                
                clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯")
        
        # äº‹ä»¶ç»‘å®š
        download_btn.click(
            download_and_load_model,
            inputs=[model_dropdown],
            outputs=[model_status, msg_input]
        ).then(
            lambda: gr.update(interactive=True),
            outputs=[send_btn]
        )
        
        refresh_btn.click(
            lambda: gr.update(choices=get_available_models()),
            outputs=[model_dropdown]
        )
        
        msg_input.submit(
            chat_response,
            inputs=[msg_input, chatbot],
            outputs=[chatbot]
        ).then(
            lambda: "",
            outputs=[msg_input]
        )
        
        send_btn.click(
            chat_response,
            inputs=[msg_input, chatbot],
            outputs=[chatbot]
        ).then(
            lambda: "",
            outputs=[msg_input]
        )
        
        clear_btn.click(
            lambda: [],
            outputs=[chatbot]
        )
    
    return app

if __name__ == "__main__":
    print(f"ğŸš€ å¯åŠ¨ {UI_CONFIG['title']}...")
    print("ğŸ“‹ åŠŸèƒ½ç‰¹æ€§:")
    print("   - è‡ªåŠ¨è·å–Hugging Faceä¸Šå°äº7Bçš„æ¨¡å‹")
    print("   - æœ¬åœ°è¿è¡Œï¼Œä¿æŠ¤éšç§")
    print("   - æ”¯æŒæ¨¡å‹ä¸‹è½½å’Œç®¡ç†")
    print("   - å‹å¥½çš„å¯¹è¯ç•Œé¢")
    print("   - å®Œæ•´çš„æ—¥å¿—è®°å½•")
    print("   - å¯é…ç½®çš„æ¨¡å‹å‚æ•°")
    print(f"\nğŸŒ æœåŠ¡å™¨å°†åœ¨ http://localhost:{SERVER_CONFIG['port']} å¯åŠ¨")
    
    app = create_interface()
    app.launch(
        server_name=SERVER_CONFIG["host"],
        server_port=SERVER_CONFIG["port"],
        share=SERVER_CONFIG["share"],
        inbrowser=SERVER_CONFIG["inbrowser"],
        show_error=SERVER_CONFIG["show_error"]
    )