# 🤖 LocalAI 对话助手

基于 `llama-cpp-python` 和 `gradio` 的本地AI对话系统，支持自动从Hugging Face下载小于7B参数的模型。

## ✨ 功能特性

- 🔄 **自动模型发现**: 自动从Hugging Face获取小于7B的GGUF格式模型
- 📥 **一键下载**: 支持模型自动下载和本地缓存
- 💬 **友好界面**: 基于Gradio的现代化Web界面
- 🔒 **隐私保护**: 完全本地运行，数据不上传
- ⚡ **高效推理**: 使用llama-cpp-python进行CPU优化推理
- 🎛️ **模型管理**: 支持多模型切换和管理

## 🚀 快速开始

### 1. 环境要求

- Python 3.8+
- 至少4GB可用内存
- 稳定的网络连接（用于下载模型）

### 2. 安装依赖

```bash
# 克隆或下载项目到本地
cd LocalAI

# 安装依赖
pip install -r requirements.txt
```

### 3. 运行程序

```bash
python app.py
```

程序启动后会自动打开浏览器，访问 `http://localhost:7860`

## 📖 使用指南

### 模型下载和加载

1. **选择模型**: 在左侧面板的下拉菜单中选择想要使用的AI模型
2. **下载模型**: 点击"📥 下载并加载模型"按钮
3. **等待加载**: 首次下载可能需要几分钟，后续使用会直接加载本地缓存
4. **开始对话**: 模型加载成功后，右侧对话框会变为可用状态

### 对话功能

- 在输入框中输入消息，按回车或点击"发送"按钮
- 支持多轮对话，系统会记住对话历史
- 点击"🗑️ 清空对话"可以重置对话历史

### 模型管理

- 点击"🔄 刷新模型列表"获取最新的可用模型
- 已下载的模型会缓存在 `models/` 目录中
- 模型配置信息保存在 `model_config.json` 文件中

## 🔧 配置说明

### 支持的模型类型

程序会自动筛选以下类型的模型：
- 参数量小于7B的模型
- GGUF格式的量化模型
- 支持对话的Chat模型

### 推荐模型

- **TinyLlama/TinyLlama-1.1B-Chat-v1.0**: 轻量级，适合低配置设备
- **Qwen/Qwen1.5-1.8B-Chat**: 中文支持较好
- **microsoft/DialoGPT-small**: 对话专用模型

## 📁 项目结构

```
LocalAI/
├── app.py                 # 主程序文件
├── requirements.txt       # 依赖列表
├── README.md             # 说明文档
├── models/               # 模型缓存目录（自动创建）
└── model_config.json     # 模型配置文件（自动创建）
```

## ⚠️ 注意事项

1. **首次运行**: 首次下载模型需要稳定的网络连接
2. **存储空间**: 每个模型大小通常在1-4GB，请确保有足够的磁盘空间
3. **内存要求**: 运行模型至少需要4GB内存，推荐8GB以上
4. **网络代理**: 如果网络访问Hugging Face有问题，可能需要配置代理

## 🛠️ 故障排除

### 常见问题

**Q: 模型下载失败怎么办？**
A: 检查网络连接，或尝试使用VPN。也可以手动下载GGUF文件到models目录。

**Q: 模型加载失败？**
A: 确保有足够的内存，尝试选择更小的模型。

**Q: 对话响应很慢？**
A: 这是正常现象，CPU推理速度较慢。可以考虑使用更小的模型或升级硬件。

### 日志查看

程序运行时会在控制台输出详细日志，遇到问题时请查看控制台信息。

## 🤝 贡献

欢迎提交Issue和Pull Request来改进这个项目！

## 📄 许可证

本项目采用MIT许可证，详见LICENSE文件。

## 🙏 致谢

- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) - 高效的LLaMA推理库
- [Gradio](https://gradio.app/) - 快速构建ML应用界面
- [Hugging Face](https://huggingface.co/) - 提供丰富的预训练模型

---

**享受与AI的对话吧！** 🎉